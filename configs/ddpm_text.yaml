train:
  batch_size: 264
  learning_rate: 8e-5
  lr_warmup_steps: 100
  num_epochs: 50
  gradient_accumulation_steps: 3
  mixed_precision: "no"
  val_iters: 49
  save_model_iters: 5000
  resume_train: false
  resume_model: "/pub0/qasim/1xgpt/humanoid_world_model/val/12-26-12-49/checkpoint-0-5"

val:
  run: true
  batch_size: 64
  skip_val_loss: False
  skip_img_sample: False

image_size: 256
debug: False
one_sample: False
exp_prefix: "DDPM"
seed: 0
log_dir: "/pub0/qasim/1xgpt/humanoid_world_model/logs"

dtype: 'bf16'
mixed_precision: "no"  # Options: ["no", "fp16", "bf16"]

model:
  type: 'unet'
  unet_blocks: [256, 512, 768, 1024, 2048]
  unet_attention_resolutions: [32, 16, 8, 4]
  noise_steps:  1000
  cfg_scale: 2
  cfg_prob: 0.2
  scheduler_type: "DDPM"
  ema_inv_gamma: 1.0
  ema_power: 0.75
  ema_max_decay: 0.999

data:
  type: "coco"
  hmwm_train_dir: "/pub0/qasim/1xgpt/data/data_v2_raw/train_v2.0_raw"
  hmwm_val_dir: "/pub0/qasim/1xgpt/data/data_v2_raw/val_v2.0_raw"
  coco_train_imgs: "/pub0/data/mscoco_2017/coco/images/train2017"
  coco_train_ann: "/pub0/data/mscoco_2017/coco/annotations/captions_train2017.json"
  coco_val_imgs: "/pub0/data/mscoco_2017/coco/images/val2017"
  coco_val_ann: "/pub0/data/mscoco_2017/coco/annotations/captions_val2017.json"

conditioning:
  type: "text"
  text_tokenizer: "CLIP"  # Options: ["T5", "CLIP"]
  prompt_file: "sample_prompts.txt"

image_tokenizer:
  path: "/pub0/qasim/1xgpt/Cosmos-Tokenizer/pretrained_ckpts"
  tokenizer_type: "CI"  # Options: ["CI", "DI"]
  spatial_compression: 8
  dtype: "bfloat16"